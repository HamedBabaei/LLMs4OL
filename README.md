
# LLMs4OL: Large Language Models for Ontology Learning 

**[LLMs4OL Paradigm](./README.md#llms4ol-paradigm-setup) | [Task A: Term Typing](./TaskA/README.md) | [Task B: Type Taxonomy Discovery](./TaskB/README.md) | [Task C: Type Non-Taxonomic Relation Extraction](./TaskB/README.md)** | **Keywords:** Zero-Shot, Few-Shot, Prompt-Learning, Ontology Learning, Large Language Models, Prompting.|

### Table of Contents
- Repository Structure
- Overview
- LLMs4OL Paradigm Setup
    - Tasks
    - Datasets
    - Results
- How to run.
    - Software Dependencies and Requirements
- Citations

## Repository Structure

## Overview
Ontology Learning (OL) addresses the challenge of knowledge acquisition and representation  in a variety of domains. Recent advances in NLP and the emergence of Large Language Models, which have shown a capability to be good at crystallizing knowledge and patterns from vast text sources, we introduced the **LLMs4OL: Large Language Models for Ontology Learning** paradigm as a empirical study of LLMs for automated construction of ontologies from various domains.  The LLMs4OL paradigm tests *Does the capability of LLMs to capture intricate linguistic relationships translate effectively to OL, given that OL mainly relies on automatically extracting and structuring knowledge from natural language text?* from three core aspects of OL as tasks. They are presented as:

- Task A: Term Typing
- Task B: Type Taxonomy Discovery
- Task C: Type Non-Taxonomic Relation Extraction

![LLMs4OL](images/LLMs4OL.jpg)  
<div align="center">Figure 1: LLMs4OL Paradigm Experimental Setup</div>

## LLMs4OL Paradigm Setup

<!-- 
This repository aims to foster constructing the ORKG using predefined set of predicates existing in the graph.
This directs ORKG users to converge towards selecting predicates added by domain experts while not preventing
them from adding new ones / selecting other ones, as the crowdsourcing concept of the ORKG suggests. Note that this
service and the
Templates Recommendation service
serve the same purpose, but from different perspectives. -->

### Tasks:

### Datasets:

### Results:



## How to run

### Requirements
