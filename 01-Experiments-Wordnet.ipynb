{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0699a80-cd40-4270-aa7b-e2c40be8c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration import BaseConfig\n",
    "from datahandler import DataReader, DataWriter\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "config = BaseConfig(version=2).get_args(db_name=\"wn18rr\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# model = BertForMaskedLM.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499e72f4-ca55-4701-b9d7-6dee9dee9097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REL shape:(2995, 7), ENT shape:‌(5138, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__healthy_JJ_1</td>\n",
       "      <td>JJ</td>\n",
       "      <td>having or indicating good health in body or mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__unbelievable_JJ_1</td>\n",
       "      <td>JJ</td>\n",
       "      <td>beyond belief or understanding; \"at incredible...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                entity type                                         definition\n",
       "0       __healthy_JJ_1   JJ  having or indicating good health in body or mi...\n",
       "1  __unbelievable_JJ_1   JJ  beyond belief or understanding; \"at incredible..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_test  = DataReader.load_df(config.processed_test)\n",
    "en_test  = DataReader.load_df(config.processed_entity_test)\n",
    "\n",
    "print(f\"REL shape:{re_test.shape}, ENT shape:‌{en_test.shape}\")\n",
    "en_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed0da0-e1c2-4ced-8790-44a851a8e84b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c17afe-05cb-4cea-9ac3-78c6469ed6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_templates = {\n",
    "    \"template-1\": \"The word [A] POS is a [MASK].\",\n",
    "    \"template-2\": \"The word '[A]' POS is a [MASK].\",\n",
    "    \"template-3\": \"[EXAMPLE]. The word [A] POS is a [MASK].\",\n",
    "    \"template-4\": \"[EXAMPLE]. The word '[A]' POS is a [MASK].\"\n",
    "}\n",
    "\n",
    "label_mapper = {\"JJ\":\"adjective\", \"NN\":\"noun\", \"VB\":\"verb\"}\n",
    "\n",
    "wn_types_identifier = {\"J\": wn.ADJ, \"V\":wn.VERB, \"N\": wn.NOUN}\n",
    "\n",
    "def make_prediction(sentence, top_n = 5):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        token_logits = model(**inputs).logits\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_n_tokens = torch.topk(mask_token_logits, top_n, dim=1).indices[0].tolist()\n",
    "    predictions = []\n",
    "    for token in top_n_tokens:\n",
    "        predictions.append(tokenizer.decode([token]))\n",
    "    return predictions, mask_token_logits\n",
    "\n",
    "def precision_at_k(actual, predicted):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted)\n",
    "    result = len(act_set & pred_set) / float(len(predicted))\n",
    "    return result\n",
    "\n",
    "def recall_at_k(actual, predicted):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted)\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def generate_samples_label_entity(sample, template=\"template-1\"):\n",
    "    concept, wn_type = \" \".join(sample.split(\"_\")[2:-2]), sample.split(\"_\")[-2]\n",
    "    label = [label_mapper[wn_type]]\n",
    "    sentence_template = ent_templates[template]     \n",
    "    sentence = sentence_template.replace(\"[A]\", concept)\n",
    "    if template == \"template-3\" or template == \"template-4\":\n",
    "        prefix = concept\n",
    "        synsets = wn.synsets(\"_\".join(concept.split()), pos=wn_types_identifier[wn_type[0]])\n",
    "        if len(synsets) != 0:\n",
    "            example = synsets[0].examples()\n",
    "            if len(example) != 0:\n",
    "                prefix = example[0]\n",
    "        sentence = sentence.replace(\"[EXAMPLE]\", prefix)\n",
    "    return sentence, label\n",
    "\n",
    "def fill_mask_prediction(entity_list, template, top_n=5):\n",
    "    P1, P5 = [], []\n",
    "    R1, R5 = [], []\n",
    "    for entity in tqdm(entity_list):\n",
    "        sentence, label = generate_samples_label_entity(entity, template=template)\n",
    "        predictions, _ = make_prediction(sentence, top_n=top_n)\n",
    "        P1.append(precision_at_k(label, predictions[:1]))\n",
    "        P5.append(precision_at_k(label, predictions))\n",
    "        R1.append(recall_at_k(label, predictions[:1]))\n",
    "        R5.append(recall_at_k(label, predictions))\n",
    "\n",
    "    print(f\"ENT-Template: {ent_templates[template]}, Test Size:{len(entity_list)} ,MP@1 = {sum(P1)/len(P1)},   MR@1 ={sum(R1)/len(R1)}\")\n",
    "    print(f\"ENT-Template: {ent_templates[template]}, Test Size:{len(entity_list)} ,MP@{top_n} = {sum(P5)/len(P5)},   MR@{top_n} ={sum(R5)/len(R5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c717ea93-1c5f-4c06-8159-c0e111ab06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template-1\n",
      "sent: The word covering POS is a [MASK].\n",
      "label: ['noun']\n",
      "template-2\n",
      "sent: The word 'covering' POS is a [MASK].\n",
      "label: ['noun']\n",
      "template-3\n",
      "sent: under a covering of dust. The word covering POS is a [MASK].\n",
      "label: ['noun']\n",
      "template-4\n",
      "sent: under a covering of dust. The word 'covering' POS is a [MASK].\n",
      "label: ['noun']\n"
     ]
    }
   ],
   "source": [
    "top_n = 1\n",
    "entity = \"__covering_NN_1\"\n",
    "\n",
    "print(\"template-1\")\n",
    "sentence, label = generate_samples_label_entity(entity, template=\"template-1\")\n",
    "print(\"sent:\", sentence)\n",
    "print(\"label:\", label)\n",
    "\n",
    "print(\"template-2\")\n",
    "sentence, label = generate_samples_label_entity(entity, template=\"template-2\")\n",
    "print(\"sent:\", sentence)\n",
    "print(\"label:\", label)\n",
    "\n",
    "print(\"template-3\")\n",
    "sentence, label = generate_samples_label_entity(entity, template=\"template-3\")\n",
    "print(\"sent:\", sentence)\n",
    "print(\"label:\", label)\n",
    "\n",
    "print(\"template-4\")\n",
    "sentence, label = generate_samples_label_entity(entity, template=\"template-4\")\n",
    "print(\"sent:\", sentence)\n",
    "print(\"label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7edf6-1cc7-4fa3-a4b6-3b6f52b41768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1) Template-1: `The word [A] POS is a [MASK].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b09606e1-d1ac-4253-a974-8ec30cda374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475100a136434a4885933fb9ef51a15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT-Template: The word [A] POS is a [MASK]., Test Size:5138 ,MP@1 = 0.0017516543402101986,   MR@1 =0.0017516543402101986\n",
      "ENT-Template: The word [A] POS is a [MASK]., Test Size:5138 ,MP@5 = 0.14332425068120483,   MR@5 =0.7166212534059946\n"
     ]
    }
   ],
   "source": [
    "entities = en_test['entity'].tolist()\n",
    "\n",
    "fill_mask_prediction(entity_list = entities, template=\"template-1\", top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2699482-d3b4-4a16-ac5d-bb18b9ca9cda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2) Template-2: `The word '[A]' POS is a [MASK].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8edfac30-10b0-4cf3-a30c-0f9662c45845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f0dcd2406c41ceae0aa05cf089fe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT-Template: The word '[A]' POS is a [MASK]., Test Size:5138 ,MP@1 = 0.00019462826002335538,   MR@1 =0.00019462826002335538\n",
      "ENT-Template: The word '[A]' POS is a [MASK]., Test Size:5138 ,MP@5 = 0.14896847022188342,   MR@5 =0.7448423511093811\n"
     ]
    }
   ],
   "source": [
    "entities = en_test['entity'].tolist()\n",
    "\n",
    "fill_mask_prediction(entity_list = entities, template=\"template-2\", top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bdcbd-be9f-41c0-9952-04e46356b534",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3) Template-3: `[EXAMPLE]. The word [A] POS is a [MASK].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e1431c6-fd21-4900-b9c8-03f4dc37188a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b077d5e6a3714432b9edf1ab0805581d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT-Template: [EXAMPLE]. The word [A] POS is a [MASK]., Test Size:5138 ,MP@1 = 0.014207862981704943,   MR@1 =0.014207862981704943\n",
      "ENT-Template: [EXAMPLE]. The word [A] POS is a [MASK]., Test Size:5138 ,MP@5 = 0.13219151420786637,   MR@5 =0.660957571039315\n"
     ]
    }
   ],
   "source": [
    "entities = en_test['entity'].tolist()\n",
    "\n",
    "fill_mask_prediction(entity_list = entities, template=\"template-3\", top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebdf83-2fc1-4222-ab87-1219049be9b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4) Template-4: `[EXAMPLE]. The word '[A]' POS is a [MASK].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5be51dfe-4a78-41ba-853b-f1fa92a32aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651bbbaf44d244dcb711fd5b7c027b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT-Template: [EXAMPLE]. The word '[A]' POS is a [MASK]., Test Size:5138 ,MP@1 = 0.10685091475282212,   MR@1 =0.10685091475282212\n",
      "ENT-Template: [EXAMPLE]. The word '[A]' POS is a [MASK]., Test Size:5138 ,MP@5 = 0.16340988711561968,   MR@5 =0.8170494355780459\n"
     ]
    }
   ],
   "source": [
    "entities = en_test['entity'].tolist()\n",
    "\n",
    "fill_mask_prediction(entity_list = entities, template=\"template-4\", top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2035d2-7f88-47df-a11d-eb96db9589d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Check Number of entities with example in `WordNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f525c89-83fd-4c73-ae5f-8a7238bf31f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b4acf053ff4a5f9f7715c46bbb36e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WN contains 1903 example for 5138 entities.\n"
     ]
    }
   ],
   "source": [
    "def check_concept_in_wordnet(sample):\n",
    "    synsets = wn.synsets(\"_\".join(sample.split(\"_\")[2:-2]), \n",
    "                         pos=wn_types_identifier[sample.split(\"_\")[-2][0]])\n",
    "    if len(synsets) != 0:\n",
    "        if len(synsets[0].examples()) != 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "entities = en_test['entity'].tolist()\n",
    "contain_example = 0\n",
    "for entity in tqdm(entities):\n",
    "    if check_concept_in_wordnet(entity):\n",
    "        contain_example += 1\n",
    "\n",
    "print(f\"WN contains {contain_example} example for {len(entities)} entities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c069a2-17b2-4418-8287-7304bde42c4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5) FreqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a3af26-9879-4643-a2b4-a3b2eb9dc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class WNFrqDataset:\n",
    "    def __init__(self, df, label_mapper, template, wn_types_identifier, is_train):\n",
    "        self.data = df\n",
    "        self.label_mapper = label_mapper\n",
    "        self.template = template\n",
    "        self.wn_types_identifier = wn_types_identifier\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data['entity'].tolist()[index]\n",
    "        concept, wn_type = \" \".join(sample.split(\"_\")[2:-2]), sample.split(\"_\")[-2]\n",
    "        label = self.label_mapper[wn_type]\n",
    "        sentence = self.template.replace(\"[A]\", concept)\n",
    "        prefix = concept\n",
    "        synsets = wn.synsets(\"_\".join(concept.split()), pos=self.wn_types_identifier[wn_type[0]])\n",
    "        if len(synsets) != 0:\n",
    "            example = synsets[0].examples()\n",
    "            if len(example) != 0:\n",
    "                prefix = example[0]\n",
    "        sentence = sentence.replace(\"[EXAMPLE]\", prefix)\n",
    "        if self.is_train:\n",
    "            sentence = sentence.replace(\"[MASK]\", \"'\"+label+\"'\")\n",
    "        return sentence, label\n",
    "\n",
    "\n",
    "class FreqModel:\n",
    "    def __init__(self, labels=['verb', 'adjective', 'noun']):\n",
    "        self.transformer = CountVectorizer(ngram_range=(1,1))\n",
    "        self.labels = labels\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        texts = [text for text, _ in dataset]\n",
    "        self.freq_matrix_labels = np.array([label for _, label in dataset])\n",
    "        self.transformer.fit(texts)\n",
    "        self.freq_matrix = self.__transform(texts)\n",
    "    \n",
    "    def predict(self, dataset):\n",
    "        # texts = [text for text, _ in range(dataset)]\n",
    "        X = [text for text, _ in dataset]\n",
    "        y_true = [label for _, label in dataset]\n",
    "        dataset_freq_matrix = self.__transform(X)\n",
    "        proba_matrix = np.zeros((dataset_freq_matrix.shape[0], len(self.labels)))\n",
    "        for index, label in enumerate(self.labels):\n",
    "            class_wise_tokens_freq = self.freq_matrix[np.where(self.freq_matrix_labels == label)[0]].sum(axis=0)\n",
    "            probas = dataset_freq_matrix/class_wise_tokens_freq\n",
    "            predict_proba = np.ma.masked_invalid(probas).sum(axis=1)\n",
    "            proba_matrix[:, index] = predict_proba.data[:]\n",
    "        y_pred = [self.labels[pred] for pred in np.argmax(proba_matrix, axis=1)]\n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def __transform(self, X):\n",
    "        return self.transformer.transform(X).toarray()\n",
    "    \n",
    "def calculate_p1(y_true, y_pred):\n",
    "    p = [precision_at_k([true], [pred]) for true, pred in zip(y_true, y_pred)]\n",
    "    return sum(p)/len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2231a1b-0f50-432a-8e78-5bf2c0dda30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23546/2054156570.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  en_train = en_train.append(en_valid).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:(23701, 3), test shape:‌(5138, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23546/2922370957.py:52: RuntimeWarning: divide by zero encountered in divide\n",
      "  probas = dataset_freq_matrix/class_wise_tokens_freq\n",
      "/tmp/ipykernel_23546/2922370957.py:52: RuntimeWarning: invalid value encountered in divide\n",
      "  probas = dataset_freq_matrix/class_wise_tokens_freq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set MP@1: 0.4739198131568704\n",
      "Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   adjective       0.09      0.87      0.17       105\n",
      "        noun       0.82      0.48      0.60      3700\n",
      "        verb       0.29      0.44      0.35      1333\n",
      "\n",
      "    accuracy                           0.47      5138\n",
      "   macro avg       0.40      0.59      0.37      5138\n",
      "weighted avg       0.67      0.47      0.53      5138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23546/2922370957.py:52: RuntimeWarning: divide by zero encountered in divide\n",
      "  probas = dataset_freq_matrix/class_wise_tokens_freq\n",
      "/tmp/ipykernel_23546/2922370957.py:52: RuntimeWarning: invalid value encountered in divide\n",
      "  probas = dataset_freq_matrix/class_wise_tokens_freq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set MP@1: 0.5579089489894942\n",
      "Train Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   adjective       0.14      0.84      0.24      1130\n",
      "        noun       0.78      0.74      0.76     13634\n",
      "        verb       0.56      0.25      0.34      8937\n",
      "\n",
      "    accuracy                           0.56     23701\n",
      "   macro avg       0.49      0.61      0.45     23701\n",
      "weighted avg       0.67      0.56      0.58     23701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_train  = DataReader.load_df(config.processed_entity_train)\n",
    "en_valid  = DataReader.load_df(config.processed_entity_valid)\n",
    "en_train = en_train.append(en_valid).reset_index(drop=True)\n",
    "en_test  = DataReader.load_df(config.processed_entity_test)\n",
    "\n",
    "print(f\"train shape:{en_train.shape}, test shape:‌{en_test.shape}\")\n",
    "\n",
    "freq_train_dataset = WNFrqDataset(df=en_train, \n",
    "                                  label_mapper=label_mapper, \n",
    "                                  template=ent_templates[\"template-4\"], \n",
    "                                  wn_types_identifier=wn_types_identifier,\n",
    "                                  is_train = True)\n",
    "\n",
    "freq_test_dataset = WNFrqDataset(df=en_test, \n",
    "                                 label_mapper=label_mapper, \n",
    "                                 template=ent_templates[\"template-4\"], \n",
    "                                 wn_types_identifier=wn_types_identifier,\n",
    "                                 is_train = False)\n",
    "\n",
    "freq_model = FreqModel()\n",
    "\n",
    "freq_model.fit(freq_train_dataset)\n",
    "\n",
    "y_true, y_pred = freq_model.predict(freq_test_dataset)\n",
    "\n",
    "print(\"Test Set MP@1:\", calculate_p1(y_true, y_pred)) \n",
    "print(\"Test Classification Report: \\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "y_true, y_pred = freq_model.predict(freq_train_dataset)\n",
    "print(\"Train Set MP@1:\", calculate_p1(y_true, y_pred)) \n",
    "print(\"Train Classification Report: \\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2297b7-9917-418e-8640-df4bdb97a089",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a02ff-2825-4a9b-9ce1-cc19a3e394a5",
   "metadata": {},
   "source": [
    "|#| Template |Model|Set | Test Size | MAP@1 | MAP@5 | |\n",
    "|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "|1|`The word [A] POS is a [MASK].`|BERT-Large|Entity Test| 5138 |  0.175|14.332||\n",
    "|2|`The word '[A]' POS is a [MASK].`|BERT-Large|Entity Test| 5138 |  0.019 | 14.896 ||\n",
    "|3|`[EXAMPLE]. The word [A] POS is a [MASK].`| BERT-Large | Entity Test | 5138 | 1.420 | 13.219 |Examples are from WordNet. WN contain examples for 1903 entities|\n",
    "|4|`[EXAMPLE]. The word '[A]' POS is a [MASK].`| BERT-Large | Entity Test | 5138 | 10.685| 16.340 |Examples are from WordNet. WN contain examples for 1903 entities|\n",
    "|5|`[EXAMPLE]. The word '[A]' POS is a [MASK].`| Freq Based Model | Entity Test | 5138 | 47.391 | | This is frequency based probability model. I just calculated tokens probaility for each class and sum them over appeared vocabulary probaility in samples for each class|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c767390-1e68-4089-9ff0-6ded10ba17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mtx = np.array([[1.14835892],\n",
    " [1.07693035],\n",
    " [0.57433328],\n",
    " [0.04026681],\n",
    " [0.04684576],\n",
    " [1.54026681],\n",
    " [0.37433328],\n",
    " [0.62360014],\n",
    " [0.37360014],\n",
    " [0.37360014]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1313a52-be88-4c95-9606-27d5c14463b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14835892, 1.07693035, 0.57433328, 0.04026681, 0.04684576,\n",
       "       1.54026681, 0.37433328, 0.62360014, 0.37360014, 0.37360014])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx.reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18397fb-d82a-478d-9db4-40ff7cd8d97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
